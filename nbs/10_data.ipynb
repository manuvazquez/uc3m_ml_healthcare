{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7cc0a-0642-48a4-a598-dc2e92f75b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3739-1d1f-4ecd-beca-2193cec32a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472b326-73a3-497b-abe1-4112c70fa1e6",
   "metadata": {},
   "source": [
    "# data\n",
    "> Data reading and wrangling functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d302f-cca2-4715-b579-fa31d1790412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import types\n",
    "\n",
    "from fastcore.utils import patch\n",
    "\n",
    "import torch\n",
    "import torch.distributions\n",
    "from torch.distributions import uniform\n",
    "\n",
    "# from latent_ode.generate_timeseries import Periodic_1d\n",
    "from uc3m_ml_healthcare.generate_timeseries import Periodic_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25eeb7-0ab0-4703-81b2-fa9865bf77fe",
   "metadata": {},
   "source": [
    "These `import`s are not actually required by this module but only used in tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20d3e2-d7a2-44e0-b59a-f053da80d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9dfb6-f8aa-4251-b9b7-4caa7a9c6a2d",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54d4b1-f375-4993-afa0-2f8f04a1c87d",
   "metadata": {},
   "source": [
    "Except for the first and last lines, everything else comes from [Rubanova's implementation](https://github.com/YuliaRubanova/latent_ode/blob/c0682d4f52b806fb88d965755892eadd9783f936/lib/parse_datasets.py) (comments mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b138b6-14fa-430b-a6b8-878b3c336111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_periodic_dataset(\n",
    "    timepoints: int, # Number of time instants\n",
    "    extrap: bool, # Whether extrapolation is peformed\n",
    "    max_t: float, # Maximum value of time instants\n",
    "    n: int, # Number of examples\n",
    "    noise_weight: float # Standard deviation of the noise to be added\n",
    "): # Time and observations\n",
    "# ) -> tuple[torch.Tensor, torch.Tensor]: # Time and observations # <-------------- Python 3.10\n",
    "    \n",
    "    # so that we can use the original code \"verbatim\" (plus some comments)\n",
    "    args = types.SimpleNamespace(timepoints=timepoints, extrap=extrap, max_t=max_t, n=n, noise_weight=noise_weight)\n",
    "    \n",
    "    # --------- Rubanova (see above)\n",
    "\n",
    "    n_total_tp = args.timepoints + args.extrap\n",
    "\n",
    "    # better understood as max_t_extrap = n_total_tp / args.timepoints * args.max_t (you adjust `max_t` if extrapolation is requested)\n",
    "    # if `args.extrap` is `False`, then this is exactly equal to `n_total_tp` since `n_total_tp = args.timepoints`\n",
    "    max_t_extrap = args.max_t / args.timepoints * n_total_tp\n",
    "\n",
    "    distribution = uniform.Uniform(torch.Tensor([0.0]),torch.Tensor([max_t_extrap]))\n",
    "    time_steps_extrap =  distribution.sample(torch.Size([n_total_tp-1]))[:,0] # last part is just \"squeezing\"\n",
    "    time_steps_extrap = torch.cat((torch.Tensor([0.0]), time_steps_extrap)) # 0 is always there\n",
    "    time_steps_extrap = torch.sort(time_steps_extrap)[0]\n",
    "\n",
    "    # frequencies are not passed (and henced sampled internally)\n",
    "    dataset_obj = Periodic_1d(\n",
    "        init_freq = None, init_amplitude = 1.,\n",
    "        final_amplitude = 1., final_freq = None, \n",
    "        z0 = 1.)\n",
    "\n",
    "    dataset = dataset_obj.sample_traj(time_steps_extrap, n_samples = args.n, noise_weight = args.noise_weight)\n",
    "    \n",
    "    # ---------\n",
    "    \n",
    "    return time_steps_extrap, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c18f4-27dc-48e3-aded-ecd0c39ffae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([101]), torch.Size([200, 101, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time, observations = make_periodic_dataset(timepoints=100, extrap=True, max_t=5.0, n=200, noise_weight=0.01)\n",
    "time.shape, observations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e57dc-272c-401a-93d9-51bb2dafdf43",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356b6f8-6fe3-4c1a-bfe2-a6ff4cb24060",
   "metadata": {},
   "source": [
    "A class defining a (somehow complex) *collate function* for a PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89a4ee-a926-448b-8afd-67a36f26b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CollateFunction:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 time: torch.Tensor, # Time axis [time]\n",
    "                 # n_points_to_subsample: int | None = None # Number of points to be \"subsampled\" # <-------------- Python 3.10\n",
    "                 n_points_to_subsample = None # Number of points to be \"subsampled\"\n",
    "                ):\n",
    "        \n",
    "        self.time = time\n",
    "        \n",
    "        self._n_time_instants = len(time)\n",
    "        self._half_n_time_instants: int = self._n_time_instants // 2\n",
    "        \n",
    "        if n_points_to_subsample is None:\n",
    "            self.n_points_to_subsample = self._half_n_time_instants\n",
    "        else:\n",
    "            self.n_points_to_subsample = n_points_to_subsample\n",
    "            \n",
    "    \n",
    "    # TODO: drop time instants with no data (`get_next_batch` in Rubanova's code)\n",
    "        \n",
    "    def __call__(self,\n",
    "                 batch: list # Observations [batch]\n",
    "                ) -> dict:\n",
    "        \n",
    "        # [batch, time, feature]\n",
    "        batch = torch.stack(batch)\n",
    "        \n",
    "        # ----------- splitting on training and to-predict\n",
    "        \n",
    "        # for observations\n",
    "        observed_data = batch[:, :self._half_n_time_instants, :].clone()\n",
    "        to_predict_data = batch[:, self._half_n_time_instants:, :].clone()\n",
    "        \n",
    "        # for time\n",
    "        observed_time = self.time[:self._half_n_time_instants].clone()\n",
    "        to_predict_at_time = self.time[self._half_n_time_instants:].clone()\n",
    "        \n",
    "        # ----------- mask\n",
    "        \n",
    "        # CAVEAT: only on observed data\n",
    "        observed_mask = torch.ones_like(observed_data, device=observed_data.device)\n",
    "        \n",
    "        # if we are to sample ALL the points in the observed data...\n",
    "        if self._half_n_time_instants == self.n_points_to_subsample:\n",
    "            \n",
    "            # ...there is nothing to do here\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            raise Exception('not implemented')\n",
    "            \n",
    "        # ----------- observation-less time instants\n",
    "        \n",
    "        # # summing across \"batch\" and \"feature\" dimensions\n",
    "        # non_missing = (observed_data.sum(dim=(0, 2)) != 0.)\n",
    "        \n",
    "        \n",
    "        return dict(\n",
    "            observed_time=observed_time, observed_data=observed_data,\n",
    "            to_predict_at_time=to_predict_at_time, to_predict_data=to_predict_data,\n",
    "            observed_mask=observed_mask)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return f'Collate function expecting {self._n_time_instants} time instants, subsampling {self.n_points_to_subsample}.'\n",
    "    \n",
    "    # a object is represented by its string\n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def to(self, device):\n",
    "        \n",
    "        self.time = self.time.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e9d71-4779-4f66-a912-cf0b6cee270a",
   "metadata": {},
   "source": [
    "Let us build an object for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa453c-6fac-4b24-bdbe-31889506c182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collate function expecting 101 time instants, subsampling 50."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn = CollateFunction(time, n_points_to_subsample=50)\n",
    "collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ed7d2-f4f3-43c8-b3ba-79beb8d1dba8",
   "metadata": {},
   "source": [
    "We also need a PyTorch `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab193d4-5914-4057-a375-74c386b496a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(observations, batch_size = 10, shuffle=False, collate_fn=collate_fn)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb5432-d99d-4a99-93cc-480a13730cf8",
   "metadata": {},
   "source": [
    "How many batches is this `DataLoader` providing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0439c-d0cd-430e-b042-d11c73a7dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = len(dataloader)\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8071eec-49cb-48ed-9967-7e2de8e23eeb",
   "metadata": {},
   "source": [
    "Let us get the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc6b2c-712b-488d-8b81-138ac4735e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_bundle = next(iter(dataloader))\n",
    "type(batch_bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3c6b9-cd85-45e6-ad81-cf4d896cdeaf",
   "metadata": {},
   "source": [
    "Notice that, as seen from `CollateFunction.__call__` function's prototype, the type is returned is a dictionary. It contains the following fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde12f7-8979-4be1-9a53-b263e9acc10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['observed_time', 'observed_data', 'to_predict_at_time', 'to_predict_data', 'observed_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(batch_bundle.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea9495-d105-4f61-ab46-eac3345043dd",
   "metadata": {},
   "source": [
    "- `observed_time` and `observed_data` is the **first part** of a time series we want to learn, whereas\n",
    "- `to_predict_at_time`, `to_predict_data` is the **second part** of the *same* time series we aim at predicting; on the other hand\n",
    "- `observed_mask` is `True` for every observation that is available (it only applies to the *observed* data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba36b5-dcd6-4bef-ba72-5cc6091e5016",
   "metadata": {},
   "source": [
    "If one must think of this in terms of an input, $x$, that is given, and a related output, $y$, that is to be predicted, the latter would be `to_predict_data` and the former would encompass the rest of the fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c5df1-bfa1-4cf4-820a-f811250b0890",
   "metadata": {},
   "source": [
    "We can check the size of every component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cceaad5-4bdc-4a14-a8b3-0a97d7f5ae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of observed_time: (50,)\n",
      "Dimensions of observed_data: (10, 50, 1)\n",
      "Dimensions of to_predict_at_time: (51,)\n",
      "Dimensions of to_predict_data: (10, 51, 1)\n",
      "Dimensions of observed_mask: (10, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "for k, v in batch_bundle.items():\n",
    "    print(f'Dimensions of {k}: {tuple(v.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e87cad-1647-4f86-bc56-ed9a4a28e49e",
   "metadata": {},
   "source": [
    "In this simple example, every observatios is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca178dc-19c9-46f6-8143-14c4c77bf6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_bundle['observed_mask'] == 1.).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf7a6b-9a9e-4fe1-a6b4-0a8213ad1a0f",
   "metadata": {},
   "source": [
    "### GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2dd9b-7edf-4b30-a44b-aee526b14c06",
   "metadata": {},
   "source": [
    "If one wants to *move* this object to another device, this function will do that for all the relevant internal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b28a12-f6bb-4ad1-b1ec-9b69487bf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to(self: CollateFunction, device):\n",
    "    \n",
    "    self.time = self.time.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48b386-dc36-4c7c-b580-9cb4f7d92808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.doclinks import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffcf61-50c9-41fd-a47a-04e44ca8f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export('10_data.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
