{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7cc0a-0642-48a4-a598-dc2e92f75b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3739-1d1f-4ecd-beca-2193cec32a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472b326-73a3-497b-abe1-4112c70fa1e6",
   "metadata": {},
   "source": [
    "# Data\n",
    "> Data reading and wrangling functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d302f-cca2-4715-b579-fa31d1790412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "from fastcore.utils import patch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions\n",
    "\n",
    "# from latent_ode.generate_timeseries import Periodic_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b138b6-14fa-430b-a6b8-878b3c336111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_periodic_dataset(**kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    # so that we can use the original code \"verbatim\" (plus some comments)\n",
    "    args = types.SimpleNamespace(**kwargs)\n",
    "    \n",
    "    # ---------\n",
    "\n",
    "    n_total_tp = args.timepoints + args.extrap\n",
    "\n",
    "    # better understood as max_t_extrap = n_total_tp / args.timepoints * args.max_t (you adjust `max_t` if extrapolation is requested)\n",
    "    # if `args.extrap` is `False`, then this is exactly equal to `n_total_tp` since `n_total_tp = args.timepoints`\n",
    "    max_t_extrap = args.max_t / args.timepoints * n_total_tp\n",
    "\n",
    "    distribution = uniform.Uniform(torch.Tensor([0.0]),torch.Tensor([max_t_extrap]))\n",
    "    time_steps_extrap =  distribution.sample(torch.Size([n_total_tp-1]))[:,0] # last part is just \"squeezing\"\n",
    "    time_steps_extrap = torch.cat((torch.Tensor([0.0]), time_steps_extrap)) # 0 is always there\n",
    "    time_steps_extrap = torch.sort(time_steps_extrap)[0]\n",
    "\n",
    "    dataset_obj = Periodic_1d( # frequencies are not passed (and henced sampled internally)\n",
    "        init_freq = None, init_amplitude = 1.,\n",
    "        final_amplitude = 1., final_freq = None, \n",
    "        z0 = 1.)\n",
    "\n",
    "    dataset = dataset_obj.sample_traj(time_steps_extrap, n_samples = args.n, noise_weight = args.noise_weight)\n",
    "    \n",
    "    # ---------\n",
    "    \n",
    "    return time_steps_extrap, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa171aa-6e78-4457-bede-da7a36490e1e",
   "metadata": {},
   "source": [
    "These `import`s are not actually required by this module but only used in tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20d3e2-d7a2-44e0-b59a-f053da80d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ricardo.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57143e3c-a58c-4c7a-ac22-82f7d8198048",
   "metadata": {},
   "source": [
    "**<font color=\"red\">TODO</font>**\n",
    "- drop time instants with no data (`get_next_batch` in Rubanova's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89a4ee-a926-448b-8afd-67a36f26b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFunction:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 time: torch.Tensor, # Time axis [time]\n",
    "                 n_points_to_subsample: int # Number of points to be \"subsampled\"\n",
    "                ):\n",
    "        \n",
    "        self.time = time\n",
    "        self.n_points_to_subsample = n_points_to_subsample\n",
    "        \n",
    "        self._n_time_instants = len(time)\n",
    "        self._half_n_time_instants: int = self._n_time_instants // 2\n",
    "        \n",
    "    def __call__(self,\n",
    "                 batch: list # Observations [batch]\n",
    "                ):\n",
    "        \n",
    "        # [batch, time, feature]\n",
    "        batch = torch.stack(batch)\n",
    "        \n",
    "        # ----------- splitting on training and to-predict\n",
    "        \n",
    "        # for observations\n",
    "        observed_data = batch[:, :self._half_n_time_instants, :].clone()\n",
    "        to_predict_data = batch[:, self._half_n_time_instants:, :].clone()\n",
    "        \n",
    "        # for time\n",
    "        observed_time = self.time[:self._half_n_time_instants].clone()\n",
    "        to_predict_at_time = self.time[self._half_n_time_instants:].clone()\n",
    "        \n",
    "        # ----------- mask\n",
    "        \n",
    "        # CAVEAT: only on observed data\n",
    "        observed_mask = torch.ones_like(observed_data, device=observed_data.device)\n",
    "        \n",
    "        # if we are to sample ALL the points in the observed data...\n",
    "        if self._half_n_time_instants == self.n_points_to_subsample:\n",
    "            \n",
    "            # ...there is nothing to do here\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            raise Exception('not implemented')\n",
    "            \n",
    "        # ----------- observation-less time instants\n",
    "        \n",
    "        # # summing across \"batch\" and \"feature\" dimensions\n",
    "        # non_missing = (observed_data.sum(dim=(0, 2)) != 0.)\n",
    "        \n",
    "        \n",
    "        return dict(\n",
    "            observed_time=observed_time, observed_data=observed_data,\n",
    "            to_predict_at_time=to_predict_at_time, to_predict_data=to_predict_data,\n",
    "            observed_mask=observed_mask)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return f'Collate function expecting {self._n_time_instants} time instants, subsampling {self.n_points_to_subsample}.'\n",
    "    \n",
    "    # a object is represented by its string\n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def to(self, device):\n",
    "        \n",
    "        self.time = self.time.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4770eb-d523-4332-b2ef-6b29bf658f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export('10_data.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
