# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/30_train.ipynb.

# %% auto 0
__all__ = ['LatentODELoss']

# %% ../nbs/30_train.ipynb 3
import torch
from torch import nn

# %% ../nbs/30_train.ipynb 5
class LatentODELoss:
    "A loss function meant to be paired with Rubanova's `LatentODE`"
    
    def __init__(self,
                 noise_std: torch.Tensor, # Standard deviation of the noise assumed when computing the likelihood
                 prior: torch.distributions.normal.Normal # Prior distribution for the initial state
                ):
        
        self.noise_std = noise_std
        self.prior = prior
        
        self.mse_loss = nn.MSELoss()
        
    def __str__(self):
        
        return f'LatentODELoss with:\n\tnoise standard deviation = {self.noise_std}\n\tprior: {self.prior}'
    
    __repr__ = __str__
           
    def __call__(self,
        pred: torch.Tensor, # Predictions [time, trial, batch, feature]
        mean: torch.Tensor, # Mean [batch, ...]
        std: torch.Tensor, # Standard deviation [batch, ...]
        target: torch.Tensor, # Targets [batch, time, feature]
        target_mask: torch.BoolTensor, # Targets [batch, time, feature]
        kl_weight: float # KL divergence weight on the loss
    # ) -> tuple[torch.Tensor, dict]: # Loss and some extra info # <-------------- Python 3.10
    ): # Loss and some extra info


        # -------------- KL divergence
        
        # the *posterior* (observations have already been processed) distribution of the latent state at the beginning
        z0_posterior = torch.distributions.normal.Normal(mean, std)

        # [1, batch, latent feature]
        kl = torch.distributions.kl.kl_divergence(z0_posterior, self.prior)

        kl_average = kl.mean()
        
        # -------------- likelihood
        
        # we'd rather have [trial, batch, time, feature]
        pred = pred.permute([1, 2, 0, 3])

        assert pred.shape[1:] == target.shape

        # the distribution of the predictions...
        pred_distribution = torch.distributions.normal.Normal(loc=pred, scale=self.noise_std)

        # ...is used to compute the likelihood
        likelihood = pred_distribution.log_prob(target)

        n_samples = len(pred)
        
        # -------------- MSE

        with torch.no_grad():

            mse = self.mse_loss(pred, target.expand((n_samples,) + target.shape))
            
        # --------------

        loss = - (torch.masked_select(likelihood, target_mask).mean() - kl_weight * kl_average)

        return loss, dict(kl_average=kl_average, mse=mse)
