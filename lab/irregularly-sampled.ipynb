{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8abc578b-bb5b-419f-a698-190c5b44352a",
   "metadata": {},
   "source": [
    "**Team members**:\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b809fb3-4bad-42b6-a6e5-ee92c092b71f",
   "metadata": {},
   "source": [
    "# Machine Learning in Healthcare 2022-2023: Neural ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24764d67-67aa-48b7-a296-9fa75c04a96c",
   "metadata": {},
   "source": [
    "**General Instructions**:\n",
    "\n",
    "- The lab is mean to be carried out **in pairs**.\n",
    "- This notebook is the only thing you are expected to hand in.\n",
    "    - You must do it through the [corresponding task in *Aula Global*](https://aulaglobal.uc3m.es/mod/assign/view.php?id=4243786).\n",
    "    - Running the notebook should allow to *roughly* (up the variability induced by the use of random numbers) reproduce the results saved in the notebook.\n",
    "- You should hand in your work 2 weeks after the second session, i.e., the **deadline is December the 16th at 23:59**.\n",
    "- It is enough if one of the partners hands in the assignment as long as both members of the team appear in the cell above.\n",
    "\n",
    "**The task at hand**:\n",
    "\n",
    "- Follow the logic of the notebook filling in the missing `# your code` bits.\n",
    "- At the end, you are asked to perform a few extra experiments. They are meant to involve minor tweaks in the code.\n",
    "    - The answer to every question must be entered in the corresponding empty cell bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eb059-08f0-486c-b787-5449d82a7a0e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6244736-8660-4ec3-bfaa-668c3ea648aa",
   "metadata": {},
   "source": [
    "This section will make sure we have all the required software installed. You can safely skip it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b05a8e-fc95-427b-8c7e-8621a6606458",
   "metadata": {},
   "source": [
    "First of all, let us find out if we are in [Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051d5ff-58c0-41ec-9112-b6cedde14560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d290a8-b3be-43cc-b493-4426861cff30",
   "metadata": {},
   "source": [
    "If so, we only need to install a couple of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7be55-c13d-4788-b667-7ae21adc192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !pip install torchdiffeq\n",
    "    !pip install git+https://github.com/manuvazquez/uc3m_ml_healthcare@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3128e-981d-40a1-892a-7a5439214cbc",
   "metadata": {},
   "source": [
    "*Matplotlib* plots are to be embedded in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb10486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2578a8-22e4-41a4-ae84-7f02691b348c",
   "metadata": {},
   "source": [
    "Let us import some of the libraries we will be using down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722147e-d701-4ec5-81a8-8fe50e1e221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchdiffeq\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d5622-ff3d-4d9e-ad94-97572216a01e",
   "metadata": {},
   "source": [
    "## Neural ODEs for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93e02d-8f99-4e2c-a6ba-4228df88ec77",
   "metadata": {},
   "source": [
    "The goal of this lab is to *learn* a function (implemented as a Neural Network) able to predict (extrapolate) the future behaviour of a time series given a bunch of measurements thereof. In the figure below, assume that you are given the <font color='blue'>blue points</font> and your task is to predict how the series will continue at the <font color='red'>red points</font> by modeling the unknown dynamics governing the system. As an exaple, the <font color='green'>green line</font> is a $3^{rd}$-degree\n",
    "polynomial fit (on just the blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5be29-8f71-4b25-9c6a-51547418d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uc3m_ml_healthcare import plot\n",
    "\n",
    "t = torch.tensor([0, 0.2, 0.7, 0.9, 1.3, 1.5])\n",
    "y = torch.tensor([0.4, 0.8, 0.25, 0.2, 0.25, 0.32])\n",
    "observed_t, observed_y, to_predict_at_t, to_predict_y = t[:-2], y[:-2], t[-2:], y[-2:]\n",
    "axis = plot.partially_observed_time_series(observed_t, observed_y, to_predict_at_t)\n",
    "\n",
    "# polynomial fitting\n",
    "poly = np.polynomial.polynomial.Polynomial.fit(observed_t, observed_y, deg=3)\n",
    "\n",
    "# the \"learned\" polynomial is used on every time instant\n",
    "poly_y = poly(t)\n",
    "\n",
    "sns.lineplot(x=t, y=poly_y, color='green', linestyle='dashed', ax=axis)\n",
    "sns.scatterplot(x=to_predict_at_t, y=to_predict_y, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b558c7-0316-4261-9542-d3aad0130030",
   "metadata": {},
   "source": [
    "You could do better using more sophisticated approaches, but in any case notice we are trying to predict *out-of-domain*, i.e., on regions of the input which were never seen during training. Also, in this example we have *scalar* observations but we might have to deal with vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda5a44-92c2-4848-b888-edd93d044f0c",
   "metadata": {},
   "source": [
    "We aim at implementing a *simplified* (but equally performant) version the model for `periodic` dataset proposed in [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da315be-6b97-4cd2-9ad6-df93fcf2d5f8",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd11d05-099f-4672-af51-afbeb46a4d45",
   "metadata": {},
   "source": [
    "Let us use the code in `uc3m_ml_healthcare.data`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66afba-5764-4f71-ab7f-00af67d90349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uc3m_ml_healthcare import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ac186-60d7-436d-b590-9647fded8895",
   "metadata": {},
   "source": [
    "...to generate some synthetic data. Specifically, we will make 1000 time series, each one with 100 *scalar* observations. All the series share a common (uniformly random) time axis, and are contaminated by Gaussian noise with zero mean and standard deviation 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5da56-e611-444e-998a-2a24377d6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time, observations = data.make_periodic_dataset(\n",
    "    timepoints=100,\n",
    "    extrap=False,\n",
    "    max_t=5.,\n",
    "    n=1_000,\n",
    "    noise_weight=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22357d84-6f22-440e-a016-3d44ed8c87d8",
   "metadata": {},
   "source": [
    "Check the dimensions of both `time` and `observations` tensors and think about the interpretation of every dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e12942-09d6-45f5-95b4-0174a3fdd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~2 LOC)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19b9a0-4143-4c08-95b7-165951a0c38e",
   "metadata": {},
   "source": [
    "Plot together a few time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c7159-fe1f-47a4-8ece-a32fb6cceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~1-2 LOC)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5aab7-8e48-4499-ba4a-2c8c7f2286cb",
   "metadata": {},
   "source": [
    "Split the `observations` (`time` stays the same) in *training* and *validation* sets. Keep 20% of the data for validation.\n",
    "\n",
    "You don't have to use the suggested names (here, `train_obs` and `valid_obs`), but if you do, some *safety* checks will be performed for you along the way. **If you want to use other names, adjust or or delete all the `assert`s and referencing code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931c588-1727-40f7-8372-4e2ca2f74488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~4 LOC)\n",
    "# ...\n",
    "# train_obs = \n",
    "# valid_obs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8c41f-5359-43ea-82d5-0e192cadf15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert observations.shape[1:] == train_obs.shape[1:] == valid_obs.shape[1:],\\\n",
    "'Training and validation tensor\\'s dimension should only differ from that of the original observations in the first dimension'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18f32e-88fe-442f-b19b-29a4f5127b82",
   "metadata": {},
   "source": [
    "For every time series, we are going to simulate the above <font color='blue'>blue</font> vs <font color='red'>red</font> points situation: the first **half** of the series is <font color='blue'>observed</font> and will be used for training, while the second one is to be <font color='red'>predicted</font> (given the corresponding time instants) based on the learned model. Hence, we will break down every time series into four variables\n",
    "- time instants associated with the observations (**x**-coordinates for the <font color='blue'>blue</font> points above),\n",
    "- observations (**y**-coordinates for the <font color='blue'>blue</font> points above),\n",
    "- time instants at which we want to predict (**x**-coordinates for the <font color='red'>red</font> points above), and\n",
    "- values to be predicted (**y**-coordinates for the <font color='red'>blue</font> points above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9cf7b-407f-44a4-ad6e-0f21242ede9b",
   "metadata": {},
   "source": [
    "### Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6578d-2db7-450a-b3c8-df17b2296e35",
   "metadata": {},
   "source": [
    "The `data` module in `uc3m_ml_healthcare` provides a *collate function* to be employed with a PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) that does this on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5595ff-ce06-4dd8-b5fd-ff8da6b49d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = data.CollateFunction(time)\n",
    "collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098fd76-adb9-4890-8a40-fe828d2e0586",
   "metadata": {},
   "source": [
    "Build `DataLoader`s for both the training and validation datasets with a *batch size* of 64 examples, and using the above *collate function*. The `dataset` to be used when instantiating `torch.utils.data.DataLoader` is simply the observations, the `time` information being taken care of by the *collate function* (you can think of it as a fixed parameter embedded in the latter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2665c6-25ba-4c16-a641-387f9b20e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~3 LOC)\n",
    "# ...\n",
    "# train_dataloader = \n",
    "# valid_dataloader ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74400d15-6a2d-4509-a22a-cafe307a9631",
   "metadata": {},
   "source": [
    "Let us request a batch a take a look at what is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3383b-5974-4aad-9cf8-fbcfc391c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "type(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68974cd0-5ee7-42b7-89e2-881fc128119b",
   "metadata": {},
   "source": [
    "The above *collate function* packs a batch into a dictionary with the following fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4e3e5-9cc2-4335-91dd-afdad38e0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8c07a-619b-494c-98e9-b825d6405dc6",
   "metadata": {},
   "source": [
    "The four first fields correspond with the above-mentioned four variables into which we are going to decompose a time series. We will be ignoring the last one, `observed_mask`, in this lab. Check the shapes of the fields and make sure you know how to interpret every dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c50be-716c-4255-8de9-68475a01c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch.items():\n",
    "    print(f'{k}: {v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0754c-a483-406d-8c5b-1925895172cb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0e6fc-5da4-4354-9c5f-f1c138547b01",
   "metadata": {},
   "source": [
    "We will be building an encoder-decoder architecture. For every time series, we will *encode* (summarize) the <font color='blue'>observations</font> to get a vector, $\\mathbf{z}_0$ (in a latent space) that will completely determine (in *decoding*) the <font color='red'>values</font> to be predicted (at the given time instants). Hence, the full model will encompass a *recognizer*, the encoder, and a *generator* (the decoder). Both *modules* will use an ODE solver for propagating a latent state. We don't need to implement this ourselves as this is provided by [torchdiffeq](https://github.com/rtqichen/torchdiffeq)'s `odeint` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be216f-e1ce-4811-b179-f963b536ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchdiffeq.odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3dde8-7dff-401d-9bd3-22892cdaa8a0",
   "metadata": {},
   "source": [
    "As can be seen from the prototype of the function, we need to pass at least three arguments to `odeint`:\n",
    "- the function defining the ODE, i.e., the $f$ in \n",
    "$$\\frac{dy}{dt} = f(t,y),$$\n",
    "- the initial value, $y_0$, and\n",
    "- a sequence of time instants, $t_0, t_1, t_2\\cdots$ at which we want the solution of the ODE, $y_1, y_2, \\cdots$\n",
    "\n",
    "Let's give a try!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e14ce9-812b-4f6f-beeb-df7208a8243c",
   "metadata": {},
   "source": [
    "We make up a simple function, `f`, an initial value, `y0`, and some time instants, `t`, at which we would like to *approximate* the value of $y$. Notice how we encapsulate everything in `torch.tensor` since `torchdiffeq` is meant to be used with *PyTorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190a2ca-d6ba-48d4-b30b-b225e4499236",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda t, y : 2*t\n",
    "y0 = torch.tensor(0.)\n",
    "t = torch.tensor([0.1, 0.5, 1., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a82701-9115-45c8-a960-87b342aea153",
   "metadata": {},
   "source": [
    "Let us numerically integrate the differential equation using `odeint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc381a9-6ab0-4751-b1d7-d92dcf51fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torchdiffeq.odeint(f, y0, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d0947-2f80-419b-8d72-23029a745468",
   "metadata": {},
   "source": [
    "What would you expect to see in plotting `y`? Double check by plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a6ea3-e603-487d-9440-5096926b534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~3 LOC)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75866d78-2097-4e7b-8beb-a8381f8953df",
   "metadata": {},
   "source": [
    "Notice that `odeint` expects a function (`f` above) receiving two arguments: the time, and the value of the function. Hence, never mind that your modeling function doesn't need time (it won't), you **always have to pass a function (a NN) that accepts a time instant as the first argument, and the value of the function as the second**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c180e79-b588-427d-acd1-1e518ffc708a",
   "metadata": {},
   "source": [
    "For the sake of convenience we are going to make a *custom* PyTorch `Module` that will *wrap* around a given NN and numerically integrates it using `odeint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553fb52-769e-42ec-bbdd-90f41e545e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODESolver(torch.nn.Module):\n",
    "    \"A thin wrapper around torchdiffeq's `odeint`\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 f: torch.nn.Module, # Function determining the derivative\n",
    "                 method: str = 'dopri5'\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.f = f\n",
    "        self.method = method\n",
    "        \n",
    "    def forward(self,\n",
    "                y0: torch.Tensor, # Initial value\n",
    "                t: torch.Tensor # Time instants at which to compute the solution (the first one is assumed to correspond with y0)\n",
    "               ):\n",
    "        \n",
    "        return torchdiffeq.odeint(self.f, y0, t, rtol=1e-4, atol=1e-5, method=self.method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c0895-f8c1-49be-b285-3b228e14615d",
   "metadata": {},
   "source": [
    "The above code gives you the essence of how to define a custom PyTorch `Module`: you make a class\n",
    "- inheriting from `torch.nn.Module`,\n",
    "- you define the `__init__` method as you like, but taking care of *always* call `super().__init__()` (otherwise, you don't get PyTorch's magic), and\n",
    "- you define the `forward` method, which determines the operation of the `Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc856bf-0ec2-43e4-91a7-a0fa6c9f9e45",
   "metadata": {},
   "source": [
    "Once we have instantiated the solver with the function of interest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fb499-4071-4d55-a9ba-ea1f7ca17abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ODESolver(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71778f4e-0248-4881-9612-3b7309bfcd5c",
   "metadata": {},
   "source": [
    "...we can do just as above (without the need to pass in the function at every call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7f9f3-e964-40b7-bc35-623c36758a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver(y0, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560d60f-b438-47e5-8a5c-ab90b80b42cc",
   "metadata": {},
   "source": [
    "### Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672baed-a222-4065-8e5e-bd8186de743a",
   "metadata": {},
   "source": [
    "The recognizer is going to encompass 4 different (interconnected) `torch.nn.Module`s\n",
    "- a *solver* that, in turn, requires\n",
    "- a NN modeling the derivative of the function to be integrated (the `f` above),\n",
    "- a Recurrent NN (such as [GRU](https://arxiv.org/abs/1406.1078)), and\n",
    "- another NN mapping from the latent space of the *recognizer* to that of *generator*.\n",
    "\n",
    "Let us build each one of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a787d7f-5163-433e-97b5-9e15d7661477",
   "metadata": {},
   "source": [
    "We are gonna consider a latent space for the recognizer of dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227aec4f-5cff-4fa2-af1b-c994ed904550",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_latent_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200551b-262d-438a-a0ac-5a441c294b48",
   "metadata": {},
   "source": [
    "#### A solver with its corresponding internal NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358a71b-df08-469d-a3aa-40e1d88a49de",
   "metadata": {},
   "source": [
    "Build a simple Multilayer Perceptron (MLP) with\n",
    "- `rec_latent_size` inputs, \n",
    "- `rec_latent_size` outputs,\n",
    "- 2 linear (`nn.Linear`) hidden layers of 100 units each, with\n",
    "- a *RELU* (`nn.ReLU`) after each one.\n",
    "\n",
    "The NN should expect as input (whenever the corresponding instance of the class is *called*) a time instant (recall the above requirement for `odeint`) and a `torch.Tensor` (the $y$ in $f(t,y)$). It operates by **ignoring the time instant** and sending the `torch.Tensor` sequentially across every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585c4e8-8251-4e47-a581-3faa07f59b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~10-20 LOC)\n",
    "# class MyMLP:    \n",
    "#     def __init__(self):\n",
    "#         ...\n",
    "# rec_ode_func = MyMLP(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738294d8-584e-4f0f-8034-55c3b5e39e1f",
   "metadata": {},
   "source": [
    "You should be getting something like this (you can simply `print` the variable containing the NN to check its structure). Always ignore the class and variable names that I used (here `TimeAwareMLP` and `net`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbc91c-78ce-47ae-9d04-c23305cfb113",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e977f4-846d-4ee8-bf88-8fbe919af2fa",
   "metadata": {},
   "source": [
    "Give it a try!! Our NN accepts tensors of any shape whose last dimension is of size `rec_latent_size`, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd38e6-2348-4d94-a423-a3667bfa01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = torch.randn(5, 7, rec_latent_size)\n",
    "input_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf0ff1-e929-4daa-b534-9f4b4996a473",
   "metadata": {},
   "source": [
    "The first argument is a time instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87da11f-310a-494e-810b-a55070d1706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rec_ode_func(1.2, input_example)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc22234-ee90-497f-984f-713750d9341e",
   "metadata": {},
   "source": [
    "The time instant (e.g., $3.14$ below) shouldn't matter at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0be29-00ac-4210-9596-5e286bfbd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out, rec_ode_func(3.14, input_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc174ca-ef07-44be-9744-f27554b6c91e",
   "metadata": {},
   "source": [
    "The above function will determine our solver. For the recognizer we will integrate the differential equation using Euler's method because it's faster than the default (more accurate) *dopri5* and doesn't seem to have any impact on performance (later on, you can go back here and try *dopri5* to convince yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625d548-7fff-4f1e-bf7c-889e4243a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_solver = ODESolver(rec_ode_func, method='euler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306be118-2a07-4ad8-9ff0-1d63d4747a88",
   "metadata": {},
   "source": [
    "An `ODESolver` expects as input (see above) an initial value, and a bunch of time instants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd745c-73a1-45c8-b383-52245ee29d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_example = rec_solver(input_example, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea6739-5292-4c3a-9def-51836c6eb382",
   "metadata": {},
   "source": [
    "Check the dimensions of `out` and make sure you know how they come about. A hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20728b-63e1-487c-af35-bb28c846cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output_example.shape == (len(t), ) +  input_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7823c-ef8a-4d39-97c7-8c42559adf10",
   "metadata": {},
   "source": [
    "#### A Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a1a47-616c-4aef-b01d-895d0ea019df",
   "metadata": {},
   "source": [
    "...for *assimilating* the observations. As input, it will take the observation (of size 1) and the current *state*. For us, the latter will be the mean and standard deviation of the latent state, hence **twice** `rec_latent_size`. Use [*PyTorch*'s GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html). When instantiating the class, set `batch_first` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2724c3-9817-4cf6-a727-6ff1e870b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~1 LOC)\n",
    "# rnn = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058b89e-e238-49db-86e1-d2f345712770",
   "metadata": {},
   "source": [
    "Make sure you got it right. Take a look at the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) to understand how PyTorch interprets every dimension for both the *input* and the *hidden state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73de9e-cd59-4a61-9322-292b60389f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = torch.randn(5, 3, 1)\n",
    "hidden_state_sample = torch.randn(1, 5, 2*rec_latent_size)\n",
    "\n",
    "output, next_state = rnn(input_example, hidden_state_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825607e-2e1e-44d4-b03e-0eeb446a4024",
   "metadata": {},
   "source": [
    "Let us make sure the dimensions of both tensors returned by the GRU,  `output` and `next_state` have the expected size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f42f3c-d1d6-4e07-abac-772ad0a2513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(output[:, -1, :].unsqueeze(0), next_state) # the output is simply the last state\n",
    "assert output.shape == (5, 3, 2*rec_latent_size)\n",
    "assert next_state.shape == (1, 5, 2*rec_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83ede3-2ea1-4b1e-bd9e-1b985db9bd35",
   "metadata": {},
   "source": [
    "#### A MLP for mapping between latent spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc27c05-87d3-4991-816d-2688fca32c87",
   "metadata": {},
   "source": [
    "**For the generator** we will be using a latent space of size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb2d33-5078-41b3-be50-4f7a08b1b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_latent_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb3bb9-8818-462a-8e4e-d979cfffb818",
   "metadata": {},
   "source": [
    "Define a class for a Multilayer Perceptron (MLP) with an input layer of size **twice** `rec_latent_size` (mean and standard deviation of the *recognizer*), an output layer of size **twice** `gen_latent_size` (mean and standard deviation of the *generator*), and a hidden layer with 100 units (followed by a RELU). This can be identified with `g` in the original paper. You already defined a class for an MLP above. You can copy-paste its code down here and modify it as required. Notice, however, that this MLP should **not** receive time as input, but only the latent state of the *recognizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc63385-3109-487e-b58d-544b9ffbbab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~10-20 LOC)\n",
    "# class AnotherMLP:    \n",
    "#     def __init__(self):\n",
    "#         ...\n",
    "# rec_latent_to_gen_latent = AnotherMLP(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864e720-27e3-41f5-8ddd-1c5d0fe15879",
   "metadata": {},
   "source": [
    "Give it a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0165cd-fb8e-46f3-a9bb-bacb8289964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = torch.randn(11, 4, 2*rec_latent_size)\n",
    "output = rec_latent_to_gen_latent(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b78b1-a8ea-4f9e-865c-32db3f8154a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output.shape == (11, 4, 2*gen_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330cbf38-6f4f-4bef-92fd-14eb17a851ab",
   "metadata": {},
   "source": [
    "#### Factoring all the components into a single module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13b130-8b55-4c4a-92d6-97242d8a1343",
   "metadata": {},
   "source": [
    "Let us make a new `torch.nn.Module` that combines the above pieces to implement the *recognizer*. \n",
    "\n",
    "As **inputs**, it should receive two `torch.Tensor`s:\n",
    "\n",
    "- the observations, with dimensions *[batch, time, input feature]*, and\n",
    "- the time axis.\n",
    "\n",
    "It should **output** \n",
    "\n",
    "- the mean, and \n",
    "- the standard deviation\n",
    "\n",
    "of the latent state for the *generator*, both with dimensions *[batch, generator latent feature]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd9faf-a33a-4608-86cb-4b10298af26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recognizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, solver: nn.Module, rec_to_gen: nn.Module, rnn: nn.Module):\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.solver = solver\n",
    "        self.rec_to_gen = rec_to_gen\n",
    "        self.rnn = rnn\n",
    "        \n",
    "        # your code? you can add any extra code you need/like\n",
    "    \n",
    "    def forward(self, obs, time):\n",
    "        \n",
    "        # your code (~1 LOC): find out how many examples are in this batch\n",
    "        # batch_size = \n",
    "        \n",
    "        # your code (~2 LOC): initialize the mean and standard deviation of the latent state (for the recognizer) to all zeros\n",
    "        # (the prefix `prev_` is because they will act as the \"previous\" mean and standard deviation in the loop)\n",
    "        # prev_yi_mean = \n",
    "        # prev_yi_std = \n",
    "        \n",
    "        # the time instant associated to the above (made-up) initial state (a \"little\" earlier than the first actual time instant);\n",
    "        # this is a hyperparameter\n",
    "        prev_t = time[0] - 0.01\n",
    "        \n",
    "        # let us loop through all the time instants AND observations...\n",
    "        for i_t, (t, xi) in enumerate(zip(time, obs.transpose(0, 1))):\n",
    "                \n",
    "            # the number of intermediate time instants to use for integrating between `prev_t` to `t_i` (at least 2: start and end points)\n",
    "            n_time_points = max(2, ((t - prev_t) / 0.05).int())\n",
    "\n",
    "            # your code (~1 LOC): make a tensor with `n_time_points` regularly-spaced points between `prev_t` and `t`\n",
    "            # time_points = \n",
    "            \n",
    "            assert len(time_points.shape) == 1\n",
    "\n",
    "            # your code (~1 LOC): exploit the passed in `solver` to compute the latent state at the time instants in `time_points`\n",
    "            # from the (initial) value `prev_yi_mean`\n",
    "            # ode_sol = \n",
    "            \n",
    "            assert ode_sol.shape == (n_time_points, batch_size, rec_latent_size)\n",
    "            \n",
    "            # we extract the solution at the last time instant, i.e., the latent state at `t`\n",
    "            yi_mean = ode_sol[-1]\n",
    "            \n",
    "            # your code (~1 LOC): concatenate (along the \"features dimension\") the mean, `yi_mean`, and *previous* standard deviation\n",
    "            # `prev_yi_std` in order to have a latent state that encompasses both things\n",
    "            # yi_mean_std\n",
    "            \n",
    "            # your code (~1 LOC): use the passed in `rnn` to assimilate the observations into the latent state; notice the GRU returns an output and\n",
    "            # an *updated state*; we are only interested in the latter; be careful with the dimensions of the inputs expected by the GRU (you\n",
    "            # will have to do some `unsqueeze`ing):\n",
    "            # - the `input` must have *time* as 2nd dimension (`dim=1`), and \n",
    "            # - `h_0`'s first dimension must be *layer* (we only have 1)\n",
    "            # _, updated_state = \n",
    "            \n",
    "            # for the sake of convenience, let us get rid of the time dimension (it should have size 1)\n",
    "            updated_state = updated_state.squeeze(dim=0)\n",
    "            \n",
    "            assert updated_state.shape == (batch_size, 2*rec_latent_size)\n",
    "            \n",
    "            # your code (~1 LOC): break down (across the last dimension) the *updated* latent state into mean and standard deviation for the next iteration\n",
    "            # prev_yi_mean, prev_yi_std = \n",
    "            \n",
    "            assert prev_yi_mean.shape == prev_yi_std.shape == (batch_size, rec_latent_size)\n",
    "            \n",
    "            # the current time instant becomes the previous one\n",
    "            prev_t = t\n",
    "        \n",
    "        # at this point all observations have been processed\n",
    "        \n",
    "        # notice that the above *updated* state contains the mean and standard deviation of the latent state for the recognizer\n",
    "        mean_std_y = updated_state\n",
    "        \n",
    "        # your code (~1 LOC): use the passed in `rec_to_gen` NN to map the latent state of the recognizer to that of the generator\n",
    "        # mean_std_z0 = \n",
    "        \n",
    "        assert mean_std_z0.shape[-1] == 2 * gen_latent_size\n",
    "        \n",
    "        # your code (~1 LOC): break down (along the \"features dimension\") `mean_std_z0` into mean and standard deviation\n",
    "        # mean_z0, std_z0 = \n",
    "\n",
    "        # let us enforce a non-negative standard deviation\n",
    "        std_z0 = std_z0.abs()\n",
    "\n",
    "        return mean_z0, std_z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24866f3a-2a98-48c5-9d74-f7d9c5d575d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = Recognizer(rec_solver, rec_latent_to_gen_latent, rnn)\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fec17d-f841-4a2d-a090-e37611871017",
   "metadata": {},
   "source": [
    "Let's give it a try by generating some fake data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d5fd4-702f-46ce-aca2-82c18b1f2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_example = torch.rand(batch_size, 6, 1)\n",
    "time_example = torch.linspace(0, 1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74060ee5-f421-43d1-92e4-97ba45a194e9",
   "metadata": {},
   "source": [
    "...and passing it to our new `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91278d-d29c-4e1d-9139-2fa61988ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0_mean_example, z0_std_example = rec(obs_example, time_example)\n",
    "assert z0_mean_example.shape == z0_std_example.shape == (batch_size, gen_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac702af2-58dc-495a-9273-acd7b8cd256a",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a5742-3506-4a86-b52f-bf60fb3aad4f",
   "metadata": {},
   "source": [
    "For every example, the *generator* will draw a few samples from the mean and standard deviation computed by the *recognizer*, and use another ODE solver to *push* forward each one of them to get a prediction at the requested time instants. It consists of 3 interconnected `torch.nn.Module`s:\n",
    "\n",
    "-  another (different) *solver* that, in turn, requires\n",
    "- a NN modeling the derivative of the function to be integrated (the `f`), and\n",
    "- a linear function (implemented as a `torch.nn.Module`) mapping from the latent space of *generator* to the *data space*.\n",
    "\n",
    "Let us build each one of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc52f9-7dd2-47ff-a77d-27688576a3ae",
   "metadata": {},
   "source": [
    "#### A solver with its corresponding internal NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf5796-1cab-4acc-aa23-3dddcd7918ed",
   "metadata": {},
   "source": [
    "Build another MLP with\n",
    "- `gen_latent_size` inputs, \n",
    "- `gen_latent_size` outputs,\n",
    "- 2 linear (`nn.Linear`) hidden layers of 100 units each, with\n",
    "- a *RELU* (`nn.ReLU`) after each one.\n",
    "\n",
    "Just like before, the NN should expect as input a time instant and a `torch.Tensor`. It operates by **ignoring the time instant** and sending the `torch.Tensor` sequentially across every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962140df-c148-4ab8-9c98-43bf4fb20488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code  (~10-20 LOC)\n",
    "# gen_ode_func = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e392f5-678c-4730-a0df-2f9282f9a8f0",
   "metadata": {},
   "source": [
    "Test time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839679b7-3398-48e9-9af0-1e4786e58d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = torch.randn(12, 3, gen_latent_size)\n",
    "assert gen_ode_func(0.1, input_example).shape == (12, 3, gen_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70795345-d252-45d0-9dba-c7d8633d5005",
   "metadata": {},
   "source": [
    "An ODE solver is wrapped around the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d87310-f15d-499e-8566-ee53efda3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~1 LOC)\n",
    "# gen_solver = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b239bed-8b9d-44e6-a9af-e82e4486ed11",
   "metadata": {},
   "source": [
    "The dimensions of the output have the same interpretation as before. The line below checks they are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef751c-86bd-4205-ac3e-69f8fe517736",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gen_solver(input_example, t).shape == (len(t), ) + input_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601fcbf-5444-4415-a3e2-8803e7490e49",
   "metadata": {},
   "source": [
    "#### Map from the latent space to the data space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0cf2bd-3e8b-47aa-9ed6-22c8ee3e771d",
   "metadata": {},
   "source": [
    "Let us build a simple linear function with `gen_latent_size` inputs and a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638224a6-e9d3-4f7a-8eee-dadf294737b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (~1 LOC)\n",
    "# gen_to_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3cd07-58f9-4509-9c1d-91b867905dd3",
   "metadata": {},
   "source": [
    "Are we getting the right dimensions at the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efc43e-92c7-470b-97f9-7dff369ed11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gen_to_data(input_example).shape == input_example.shape[:-1] + (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe140b1-fadc-4f63-99b9-6a00a601ad8e",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b41b0b-a0cc-4605-b71d-93d4a81770ad",
   "metadata": {},
   "source": [
    "Let us gather all the pieces into a single `torch.nn.Module` so that we simplify the *training loop* exploiting some of PyTorch's functionality.\n",
    "\n",
    "The plan is:\n",
    "\n",
    "- We use the above `recognizer` to compute a mean and standard deviation for every example in the batch.\n",
    "\n",
    "- We draw a few samples from each (assuming a Gaussian distribution)\n",
    "\n",
    "    - Be careful when drawing samples in a NN since you cannot backpropagate through the sampling process, meaning, you cannot optimize the parameters (e.g., the mean and standard deviation) of a distribution. The workaround, known as the *reparametrization trick*, consists in sampling from a distribution with fixed parameter (e.g., a standard Gaussian), and then *adjust* the samples the get the desired distribution. You don't need to implement this yourself: PyTorch distributions provide an `rsample` method that does exactly this (see the [documentation for the Gaussian](https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal.rsample)).\n",
    "    \n",
    "- We use the ODE solver to push forward each sample, and get a trajectory in the latent space.\n",
    "\n",
    "- Using `gen_to_data`, we map each trajectory in the latent space to observations onto a trajectory in the *data space*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342f839-9917-462b-b9bd-ac5e0e6e31ef",
   "metadata": {},
   "source": [
    "We will be drawing this number of samples from each example in the batch (you can play with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7049ca-c48d-4553-9f2b-8a3849fb5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f18d9-eb14-45b6-a397-f9ca922fc060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODE(nn.Module):\n",
    "    \n",
    "    def __init__(self, recognizer, solver, gen_to_data):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        self.recognizer = recognizer\n",
    "        self.solver = solver\n",
    "        self.gen_to_data = gen_to_data\n",
    "        \n",
    "        # your code? you can add any extra code you need/like\n",
    "    \n",
    "    def forward(\n",
    "        self, to_predict_at_time: torch.Tensor, observed_data: torch.Tensor, observed_time: torch.Tensor):\n",
    "        \n",
    "        # your code (~1 LOC): exploit the `recognizer` to process the observations (time and data); recall that the\n",
    "        # implemented recognizer returns both the mean and standard deviation of \"z0\"\n",
    "        # z0_mu, z0_std = \n",
    "        \n",
    "        # a standard deviation equal to zero is a problem\n",
    "        z0_std = z0_std + 1e-6\n",
    "        \n",
    "        # your code (~1 LOC): draw some samples from a Gaussian distribution\n",
    "        # z0_samples = \n",
    "        \n",
    "        # your code (~1 LOC): use the `solver` to \"push forward\" the initial state (in the latent space)\n",
    "        # trajectories = \n",
    "        \n",
    "        # your code (~1 LOC): use `gen_to_data` to map from the generator's latent space to the data space\n",
    "        # predictions = \n",
    "        \n",
    "        return predictions, z0_mu, z0_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9403c-2097-4f32-bbaa-7624cb3298e8",
   "metadata": {},
   "source": [
    "Let us instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf32eb-6615-43b1-8ce1-d84cdf56b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentODE(rec, gen_solver, gen_to_data)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b1288-b440-4487-aed8-957d4ff5dac7",
   "metadata": {},
   "source": [
    "For testing we can compute the output for the batch we set above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee48bee-4392-4e46-951a-95426cca567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06cf31-d777-4ef3-8147-057bc0a6b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61efed-8e9d-4b7c-97f3-4b8422ed5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, z0_mu, z0_std = model(batch['to_predict_at_time'], batch['observed_data'], batch['observed_time'])\n",
    "assert pred.shape == (len(time)//2, n_samples, batch_size, 1)\n",
    "assert z0_mu.shape == z0_std.shape == (batch_size, gen_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a6e7a-855c-4c3d-8d90-b75a160dad90",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c7eb4-cb26-4058-8c24-7fe9c94f5d57",
   "metadata": {},
   "source": [
    "Since we are not setting the *seeds* of the pseudo-random number generators that are being used behind the scenes (*numpy* and *PyTorch*'s), every time we run the notebook we will get different results (parameters are initialized randomly, data is generated randomly) . It shouldn't be a problem: the above model should be quite robust *almost* always find a nice fit. It's a good thing that you experiment a little bit with this to get a sense of the *variance* inherent to the training of NN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada3ed0-4ba1-421e-99fa-b6035c87bbed",
   "metadata": {},
   "source": [
    "Some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f204fe0-e5d4-4369-a539-38aeb89209ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "n_epochs = 100\n",
    "wait_until_kl_inc = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e343c35-9ace-4e2a-b6ff-7e3bbc7bcb45",
   "metadata": {},
   "source": [
    "We'll be using some code from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ebe66-5c8b-4d90-92cd-c85b4daf31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uc3m_ml_healthcare import train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ee768-0524-4f02-801f-13109e179a73",
   "metadata": {},
   "source": [
    "An optimizer and a companion scheduler with a *1cycle* learning rate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d65ab-a8e1-4c80-b2f8-986460063ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ccfa7-4d13-4c1c-bd0f-a35709bade43",
   "metadata": {},
   "source": [
    "A loss function. You can check the check [source code](https://manuvazquez.github.io/uc3m_ml_healthcare/train.html#latentodeloss) if you are interested in all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bde6e-b3bd-4919-8035-f41c957fd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0_prior = torch.distributions.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "obsrv_std = torch.tensor(0.01)\n",
    "loss_function = train.LatentODELoss(noise_std=obsrv_std, prior=z0_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97938689-f39a-4dc0-8792-d4d5d55f3de1",
   "metadata": {},
   "source": [
    "A function encapulating every operation required to process a single batch (regardless of whether it is training or validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df37262-3e3b-4495-869d-65961ee519db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model: nn.Module, batch: dict, kl_coef: float) -> tuple:\n",
    "    \n",
    "    # `model` is used to evaluate the input\n",
    "    pred_x, z0_mu, z0_std = model(batch['to_predict_at_time'], batch['observed_data'], batch['observed_time'])\n",
    "\n",
    "    to_be_predicted_mask = torch.tensor(True)\n",
    "\n",
    "    return loss_function(pred_x, z0_mu, z0_std, batch['to_predict_data'], to_be_predicted_mask, kl_weight=kl_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6d1eb-e8d0-41e5-b7c3-e7214c1f5004",
   "metadata": {},
   "source": [
    "Let's train!! Some *bumps* are expected along the way...pay them no mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac8be7-7c83-4686-8174-e7b386c99753",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for train_batch in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if the number of epochs is BELOW `wait_until_kl_inc`...\n",
    "        if epoch < wait_until_kl_inc:\n",
    "\n",
    "            # ...the KL divergence is ignored in the loss\n",
    "            kl_coef = 0.\n",
    "\n",
    "        # if the number of epochs is ABOVE `wait_until_kl_inc`...\n",
    "        else:\n",
    "\n",
    "            # ...the weight of KL divergence is adjusted\n",
    "            kl_coef = (1-0.99** (epoch - wait_until_kl_inc))\n",
    "        \n",
    "        loss, _ = process_batch(model, train_batch, kl_coef)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters are adjusted\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    # ================ validation\n",
    "    \n",
    "    # evaluation \"mode\"\n",
    "    model.eval()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        overall_valid_losses = []\n",
    "        overall_valid_mses = []\n",
    "\n",
    "        for valid_batch in valid_dataloader:\n",
    "\n",
    "            valid_loss, valid_info = process_batch(model, valid_batch, kl_coef)\n",
    "            \n",
    "            overall_valid_losses.append(valid_loss.item())\n",
    "            overall_valid_mses.append(valid_info[\"mse\"].item())\n",
    "            \n",
    "        print(f'epoch #{epoch}: loss={np.mean(overall_valid_losses)} | mse={np.mean(overall_valid_mses)}')\n",
    "        \n",
    "    # training \"mode\"\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbc60c-5944-44bd-9a21-f0c29dedeba3",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19a907-4bac-4d5f-9970-c42cbb564505",
   "metadata": {},
   "source": [
    "Let us visualize some predictions for the last batch (any will do)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586bc04-5360-4399-a510-838f151ad24b",
   "metadata": {},
   "source": [
    "We start by constructing a *finer* time axis in the prediction interval. Notice that we can request predictions in as many time instants as we like (here 120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf06a2-0c85-4f28-afc3-2296f59f26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = torch.linspace(valid_batch['to_predict_at_time'][0], valid_batch['to_predict_at_time'][-1], steps=120)\n",
    "t_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323353b4-aa2a-4f80-b872-64fd69beba4f",
   "metadata": {},
   "source": [
    "We call the `model` to get the predictions in the above interval. The dimensions of the predictions are *[time, trial, batch, input feature]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e868ad-acd4-4f7e-8858-cf897f0ee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions, *_ = model(t_pred, valid_batch['observed_data'], valid_batch['observed_time'])\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e1918-a017-4dd7-83c0-7d6cd3361b6e",
   "metadata": {},
   "source": [
    "The mean (across samples) of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ed955-0718-48ec-9562-1ff54f9008f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mean = predictions.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53794321-cf04-499b-821a-20215ac06e96",
   "metadata": {},
   "source": [
    "We rearrange the above tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb00479-59b6-47d1-b989-69590e748e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions_mean.squeeze(dim=-1).detach().numpy().T\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ca4a4-f1e7-4905-8906-962ecfb628e2",
   "metadata": {},
   "source": [
    "The *observed* and *to-be-predicted*  data in the batch are concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b5354-e02a-4a69-97e6-4a1a0735b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch = torch.concat((valid_batch['observed_time'], valid_batch['to_predict_at_time'])).detach().numpy()\n",
    "y_batch = torch.concat((valid_batch['observed_data'], valid_batch['to_predict_data']), dim=1)[...,0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe0ca0-8255-4b8f-94b5-a2fe58eb82f6",
   "metadata": {},
   "source": [
    "Let us plot only 3 time series (dots) along with the corresponding predicted trajectory in the *unseen* part of the time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55a709-fdd1-4d0f-9c11-1a25f5c3f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "fig , axis = plt.subplots()\n",
    "for i, color in zip(range(3), sns.color_palette()):\n",
    "    sns.scatterplot(x=t_batch, y=y_batch[i], color=color)\n",
    "    sns.lineplot(x=t_pred, y=y_pred[i], ax=axis, color=color)\n",
    "axis.set(xlabel='t', ylabel='observations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865625e2-dcd9-4ad1-86c1-3cdc65630909",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('predictions.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746f619-b0ba-4eba-9706-7d9aa32fd068",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fae8a-ec63-445a-96b8-b89423db0a5a",
   "metadata": {},
   "source": [
    "Try each one of the following experiments and **write down** your observations (what's the effect?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee26e75-9b04-479a-9d24-ba47ca6b254f",
   "metadata": {},
   "source": [
    "- Draw more samples of $z_0$ in the latent space of the recognizer (right now we are only drawing 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4b98b-deff-4b00-be5b-fa9b311c8539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574bd34a-80ce-470f-a316-c06e86d2b5e8",
   "metadata": {},
   "source": [
    "- Use lower-dimensional latent spaces. Try, for instance, size 4 for the *recognizer* and size 2 for the *generator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5137b0-0f5d-440f-bd16-0bcea2197c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "755db00c-ad3e-4522-8ce6-42d4182d37f0",
   "metadata": {},
   "source": [
    "- Use a different (*fixed*) initialization for the *recognizer*'s latent space (`ones` for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0708df-4fff-434f-bcb2-c114e31426fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edbe92b8-bf11-4a97-92ad-1891d2ad3eb1",
   "metadata": {},
   "source": [
    "- Remove the *regularization* based on KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d53521-2336-4ebf-9306-bfa970d96ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94def60a-9fb9-4eec-9eeb-e261c8f87111",
   "metadata": {},
   "source": [
    "- Modify the above code so that every ODE solver uses the *adjoint method* to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6384dd-a859-42d6-9cd8-95b740a2daea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
